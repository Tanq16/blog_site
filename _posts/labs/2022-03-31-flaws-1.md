---
title: flAWS 1
date: 2022-03-31 12:00:00 +0500
categories: [Lab Practice Notes, AWS Labs]
tags: [aws,lab,flaws1,security]
image:
  path: /assets/img/covers/flaws-cover.jpeg
  alt: flAWS Artwork
---

## Level 1

> This level is *buckets* of fun. See if you can find the first sub-domain.
{: .prompt-info }

Begin with doing a `dig` of `flaws.cloud`, which returns the following &rarr; 

```
; <<>> DiG 9.16.1-Ubuntu <<>> flaws.cloud
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 55678
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 4096
; COOKIE: 85e57acc1b19ec52 (echoed)
;; QUESTION SECTION:
;flaws.cloud.                   IN      A

;; ANSWER SECTION:
flaws.cloud.            15      IN      A       52.92.212.219

;; Query time: 531 msec
;; SERVER: 192.168.65.5#53(192.168.65.5)
;; WHEN: Thu Mar 31 21:09:06 CDT 2022
;; MSG SIZE  rcvd: 79
```

Then do an `nslookup` for the IP found, which gives the following &rarr; 

```
219.212.92.52.in-addr.arpa      name = s3-website-us-west-2.amazonaws.com.
Authoritative answers can be found from:
```

This gives hint of a static website running from S3 at `s3-website-us-west-2.amazonaws.com`. This also confirms that the non-DNS URL for the challenge is at `flaws.cloud.s3-website-us-west-2.amazonaws.com`.

Now, a new profile for `flaws` can be created with the region set to `us-west-2` and given that S3 is being used, permissions can be checked using the following &rarr; 

```bash
# no sign request is used to call without credentials
awsn s3 ls s3://flaws.cloud --no-sign-request --profile flaws
```

This gives the result &rarr; 

```
2017-03-13 22:00:38       2575 hint1.html
2017-03-02 22:05:17       1707 hint2.html
2017-03-02 22:05:11       1101 hint3.html
2020-05-22 13:16:45       3162 index.html
2018-07-10 11:47:16      15979 logo.png
2017-02-26 19:59:28         46 robots.txt
2017-02-26 19:59:30       1051 secret-dd02c7c.html
```

`robots.txt` and `secret-dd02c7c.html` seem interesting.

These can be retrieved using the command &rarr; 

```bash
aws s3 cp s3://flaws.cloud/robots.txt ./
```

The robots file did not give anything but the secret file gave the link to the next level as `http://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud`. Files can also be listed by directly going to `flaws.cloud.s3.amazonaws.com`.

---

## Level 2

> The next level is fairly similar, with a slight twist. You're going to need your own AWS account for this. You just need the free tier.
{: .prompt-info }

Calling `s3 ls` again on the discovered bucket gives an access denied message &rarr; 

```bash
aws s3 ls s3://level2-c8b217a33fcf1f839f6f1f73a00a9ae7.flaws.cloud --no-sign-request
```

However, calling it with a valid profile lists the bucket which means that the bucket did not have proper permissions configured i.e., it allowed everyone to list items and get items. Retrieving the secret file gives the URL for the next level as &rarr; `http://level3-9afd3927f195e10225021a578e6f78df.flaws.cloud`.

---